
##### Python implementation of Stochastic OT
##### Inspired by Harmonic Grammar implementation by Connor McLaughlin:
##### https://github.com/connormcl/harmonic-grammar


##### SOME TERMINOLOGY #####
# Overt (form): a datum that the learner hears. 
#               It may contain stress info but not foot structure info.
# Input (form): the underlying representation of an overt form
# Parse: the structural analysis of an overt form, including foot information.
#        It is the output form of a tableau corresponding to the input form.
#        The parse of an overt form varies depending on constranit ranking. 
# Generate: Compute the parse given the input and a constraint ranking.

##### SOME ABBREVIATIONS (FOR VARIABLE NAMING) #####
# constraint: const
# violation: viol
# dictionary: dict
# candidate: cand
# input: inp (to avoid overlapping with input() function)


import re
import random
import sys
import datetime
import os
import matplotlib.pyplot as plt
#from labellines import labelLine, labelLines
import time
import math

lang = sys.argv[1][:6]
syll_num = sys.argv[1][-9]

##### Part 0: Open and save grammar and target files ############################

# The command asks for two parameters: the grammar file, and whether noise is on.
# If user does not provide two parameters, throw an error.
if len(sys.argv) != 3:
    raise IndexError("Please provide all arguments required: a datum file, and whether noise is on (ON if on, OFF if off)")

if sys.argv[2] == 'ON':
    noise = True
elif sys.argv[2] == 'OFF':
    noise = False
else:
    raise ValueError("Please provide correct noise parameter: ON if on, OFF if off")

# The Grammar file is a specific format of a txt file created by Praat
# (It is called an "otgrammar" object in the Praat documentation.
grammar_file = open('ilokano_grammar.txt', 'r')
grammar_text = grammar_file.read()

# The target file is the list of overt forms to be learned by the learner.
# It is generated by extracting a certain number of overt forms from the grammar file,
# via a separate python script ("generate_learning_data.py")
#target_file = open(sys.argv[2], 'r')
target_file = open(sys.argv[1], 'r')
target_list = target_file.readlines()
target_list = [x.rstrip() for x in target_list]

# Close files
grammar_file.close()
target_file.close()


##### Part 1: Extract Information from Grammar File ############################

# Praat's otgrammar file is a list of constraints followed by a list of OT tableaux, 
# which provide information about the violation profile for each input-parse pair. 
# The format of a tableaux and its elements (input form, overt form, violation profile) 
# can be expressed in regular grammar.

### Extract list of constraints, preserving their order in grammar file
# Preserving order is important because the violation profiles in the tableaux 
# are based on this order.
const_pattern = re.compile(r"constraint\s+\[\d+\]:\s(\".*\").*")
consts = re.findall(const_pattern, grammar_text)

### Extract list of tableaux
tableau_pattern = re.compile(r"(input.*\n(\s*candidate.*\s*)+)")
tableaux_string = re.findall(tableau_pattern, grammar_text)
# Findall returns tuple b/c substring. We only need the entire string
tableaux_string = [t[0] for t in tableaux_string] 

# I define two helper functions here to use later in breaking up tableaux.
# This function combines two lists of the same length into a dictionary.
# The first list provides the keys, and the second list the values.
def map_lists_to_dict(keylist, valuelist):
    if len(keylist) != len(valuelist):
        raise ValueError("Length of lists do not match.")
    mapped_dict = {}
    for i in range(len(keylist)):
        mapped_dict[keylist[i]] = valuelist[i]
    return mapped_dict

# This function combines two lists into a list of tuples.
# The first list provides the 0th element of the tuple, the second list the 1st.
def map_lists_to_tuple_list(listone, listtwo):
    if len(listone) != len(listtwo):
        raise ValueError("Length of lists do not match.")
    mapped_list = []
    for i in range(len(listone)):
        mapped_list.append((listone[i], listtwo[i]))
    return mapped_list

# Extract violation profile for each parse for each overt form
'''
Schematic structure of a tableau in Grammar File:

{input1: {overt1: violation profile
          overt2: violation profile
          ...
          }
 input2: {overt1: violation profile
          overt2: violation profile
          overt3: violation profile
          ...
          } 
 input3: ...
}
'''    

### First, compile regex patterns for picking up inputs, overt forms, parses, and violation profile
# This picks out the input form ("|L H|")
input_pattern = re.compile(r"input\s+\[\d+\]:\s+\"(.*)\"\s+\d+") 
# This picks out the overt form ("[L1 L]"), parse ("/(L1) L/"), and violation profile ("0 1 0 ...")
# (The order of constraints is constant for all parses)
candidate_pattern = re.compile(r"candidate.*\[\d+\]\:\s+\"(\[.*\])\"\s+([\d ]+)")

# Build input_tableaux -- code very similar to overt_tableaux
input_tableaux = {}
for t in tableaux_string:
    # Since there's only one input form per tableau,
    # re.findall should always yield a list of length 1
    if len(re.findall(input_pattern, t)) != 1:
        raise ValueError("Found more than one input form in tableau. Please check grammar file.")

    inp = re.findall(input_pattern, t)[0]

    # Access the candidates again, to pick out parse and violation profile.
    candidates = re.findall(candidate_pattern, t)

    # Following for-loop is identical to overt_tableaux
    output_evals = {}
    for cand in candidates:
        output = cand[0]
        viols_string = cand[1]

        viols = viols_string.rstrip().split(' ')
        viols = [int(x) for x in viols] 

        viol_profile = map_lists_to_dict(consts, viols)
        output_evals[output] = viol_profile
        
    input_tableaux[inp] = output_evals

##### Part 2: Defining utility functions #######################################

# Add random noise to ranking values of each constraint
def add_noise(const_dict, sigma):
    const_dict_copy = const_dict.copy()
    for const in const_dict.keys():
        noise = random.gauss(0, float(sigma))
        const_dict_copy[const] = const_dict[const] + noise
    return const_dict_copy

def get_input(output):
    for inp in input_tableaux.keys():
        if output in input_tableaux[inp].keys():
            return inp
    
    raise ValueError("Could not find input for "+output+". Please check grammar file.")

# Rank constraints in const_dict by their ranking value and return an ordered list
def ranking(const_dict):
    ranked_list_raw=[]
    for const in const_dict:
        ranked_list_raw.append((const, const_dict[const]))
    random.shuffle(ranked_list_raw)
    ranked_list = sorted(ranked_list_raw, key=lambda x: x[1], reverse=True)
    ranked_list = [x[0] for x in ranked_list]
    return ranked_list

# A recursive function that does run-of-the-mill OT
# It takes as argument a special dictionary, tableau_viol_only, which acts as a sub-tableau of sorts.
def optimize(tableau_viol_only):
    # Pick out the most serious offense of each parse
    # (I.e., pick out the highest-ranked constraint violated by the parse)
    initial_batch = []
    for value in tableau_viol_only.values():
        # The value is a list of (parse, const_rank, const, viol) tuples, sorted by const_rank.
        # The first element of this list is the "most serious offense."
        initial_batch.append(value[0])

    # Among the most serious offense commited by each parse, 
    # pick out the parse(s) that committed the least serious one.
    lowest_rank_compare = []
    # max, because the *largest* const_rank value means least serious
    lowest_rank = max(initial_batch, key = lambda x:x[1])
    for parse in initial_batch:
        if parse[1] == lowest_rank[1]:
            lowest_rank_compare.append(parse)

    # If there is a single parse with the least serious offense, that's the winner.
    if len(lowest_rank_compare) == 1:
        return lowest_rank_compare[0]

    # If there are more than one least-serious offenders...
    elif len(lowest_rank_compare) > 1:
        # ... we first see whether one has violated the same constraint more than the other(s).
        viol_compare = []
        lowest_viol = min(lowest_rank_compare, key = lambda x:x[3])
        for x in lowest_rank_compare:
            if x[3] == lowest_viol[3]:
                viol_compare.append(x)

        # If there is one parse that violated the constraint the least, that's the winner.
        if len(viol_compare) == 1:
            return viol_compare[0]
        
        # If all of the least-serious offenders violated the constraint the same number of times,
        # we now need to compare their next most serious constraint offended.
        elif len(viol_compare) > 1:
            partial_tableau_viol_only = {}
            for x in viol_compare:
                # Make another tableau_viol_only with the least-serious offenders,
                # but we chuck out their most serious offenses.
                partial_tableau_viol_only[x[0]] = tableau_viol_only[x[0]][1:]
            # Run the algorithm again with the new, partial tableau_viol_only
            return optimize(partial_tableau_viol_only)
        else:
            raise ValueError("Could not find optimal candidate")
    else:
        raise ValueError("Could not find optimal candidate")

# Produce a winning parse given an input and constraint ranking
# (Basically a run-of-the-mill OT tableau)
def generate(inp, ranked_consts):
    # Pick out the constraints that *are* violated (i.e., violation > 0)
    # This "sub-dictionary" will be fed into the optimize function
    tableau_viol_only = {}
    for cand in input_tableaux[inp].keys():
        tableau_viol_only[cand] = []
        for const, viol in input_tableaux[inp][cand].items():
            if viol > 0:
                tableau_viol_only[cand].append((cand, ranked_consts.index(const), const, viol))
        tableau_viol_only[cand] = sorted(tableau_viol_only[cand], key = lambda x:x[1])

    winner = optimize(tableau_viol_only)[0]
    gen_viol_profile = input_tableaux[inp][winner]
    
    return (winner, gen_viol_profile)
        
# Adjusting the grammar, given the list of good and bad constraints
def adjust_grammar(good_consts, bad_consts, const_dict, plasticity):
    for const in good_consts:
        #const_dict[const] = const_dict[const] + float(plasticity/len(good_consts))
        const_dict[const] = const_dict[const] + float(plasticity)
    for const in bad_consts:
        const_dict[const] = const_dict[const] - float(plasticity)
    return const_dict

# In the face of an error, classify constraints into good, bad, and irrelevant constraints.
def learn(winner_viol_profile, loser_viol_profile, const_dict, plasticity):
    good_consts = [] # Ones that are violated more by the "wrong" parse than by the actual datum
    bad_consts = [] # Ones that are violated more by actual datum than by the "wrong" parse
    for const in winner_viol_profile.keys():
        if winner_viol_profile[const] > loser_viol_profile[const]:
            bad_consts.append(const)
        elif winner_viol_profile[const] < loser_viol_profile[const]:
            good_consts.append(const)
        else: # equal number of violations for the parse and the datum
            continue
    # Adjust the grammar according to the contraint classifications
    return adjust_grammar(good_consts, bad_consts, const_dict, plasticity)

##### Part 3: Learning #########################################################

# Timestamp for file
yy = str(datetime.datetime.now())[2:4]
mm = str(datetime.datetime.now())[5:7]
dd = str(datetime.datetime.now())[8:10]
hh = str(datetime.datetime.now())[11:13]
mn = str(datetime.datetime.now())[14:16]
ss = str(datetime.datetime.now())[17:19]

timestamp = yy+mm+dd+"_"+hh+mn+ss

# Designate absolute path of results file and open it
script_path = os.path.dirname(os.path.realpath(sys.argv[0])) #<-- absolute dir the script is in
results_path = script_path + '\\results'
result_file_name = "\\"+lang+"_"+str(syll_num)+"syll_"+timestamp+".txt"
result_file_path = results_path + result_file_name
results_file = open(result_file_path, 'w')

starttime = datetime.datetime.now()

# Put all constraints at 100 ranking value
constraint_dict={}
for c in consts:
    constraint_dict[c] = 100.0

# Learner will go through all words in target file, but in random order.
target_list_shuffled = random.sample(target_list, len(target_list))
target_set = set(target_list_shuffled)

# list of items learned successfully
learned_success_list = [] 

# list of ranking values per constraint
trend_tracks = {}
for const in constraint_dict.keys():
    trend_tracks[const] = [] 

### Define variables to track
# track the iteration number where change occurred
# (will plot the interval between changes)
# interval_track = [] 
# track number of learned tokens
learning_track = []
# track number of iterations for plotting
iteration_track = []

datum_counter = 0
change_counter = 0


# Actual learning loop
for t in target_list_shuffled:
    datum_counter += 1

    pair = t.split(",")
    inp = pair[0]
    target = pair[1]

    plasticity = 2.0
    sigma = 10

    if noise:    
        generation = generate(inp, ranking(add_noise(constraint_dict, sigma)))
    else:
        generation = generate(inp, ranking(constraint_dict))

    if generation[0] == target:
        #print(constraint_dict)
        #time.sleep(0.5)

        learned_success_list.append(target)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])    

    else:
        #print("ERROR: Observed "+target+" generated "+generation[0])

        # new grammar
        constraint_dict = learn(input_tableaux[inp][target], generation[1], constraint_dict, plasticity)
        # new generation with new grammar
        generation = generate(inp, ranking(constraint_dict))

        change_counter += 1

        #interval_track.append(datum_counter)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])
    
    iteration_track.append(datum_counter)
    learning_track.append(len(learned_success_list))

    if datum_counter % 1000 == 0:
        print("input "+str(datum_counter)+" out of "+str(len(target_list_shuffled))+" learned")

for t in target_list_shuffled:
    datum_counter += 1

    pair = t.split(",")
    inp = pair[0]
    target = pair[1]

    plasticity = 0.2
    sigma = 2.0

    if noise:    
        generation = generate(inp, ranking(add_noise(constraint_dict, sigma)))
    else:
        generation = generate(inp, ranking(constraint_dict))

    if generation[0] == target:
        #print(constraint_dict)
        #time.sleep(0.5)

        learned_success_list.append(target)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])    

    else:
        #print("ERROR: Observed "+target+" generated "+generation[0])

        # new grammar
        constraint_dict = learn(input_tableaux[inp][target], generation[1], constraint_dict, plasticity)
        # new generation with new grammar
        generation = generate(inp, ranking(constraint_dict))

        change_counter += 1

        #interval_track.append(datum_counter)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])
    
    iteration_track.append(datum_counter)
    learning_track.append(len(learned_success_list))

    if datum_counter % 1000 == 0:
        print("input "+str(datum_counter)+" out of "+str(len(target_list_shuffled))+" learned")

for t in target_list_shuffled:
    datum_counter += 1

    pair = t.split(",")
    inp = pair[0]
    target = pair[1]

    plasticity = 0.02
    sigma = 2.0

    if noise:    
        generation = generate(inp, ranking(add_noise(constraint_dict, sigma)))
    else:
        generation = generate(inp, ranking(constraint_dict))

    if generation[0] == target:
        #print(constraint_dict)
        #time.sleep(0.5)

        learned_success_list.append(target)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])    

    else:
        #print("ERROR: Observed "+target+" generated "+generation[0])

        # new grammar
        constraint_dict = learn(input_tableaux[inp][target], generation[1], constraint_dict, plasticity)
        # new generation with new grammar
        generation = generate(inp, ranking(constraint_dict))

        change_counter += 1

        #interval_track.append(datum_counter)

        for const in constraint_dict.keys():
            trend_tracks[const].append(constraint_dict[const])
    
    iteration_track.append(datum_counter)
    learning_track.append(len(learned_success_list))

    if datum_counter % 1000 == 0:
        print("input "+str(datum_counter)+" out of "+str(len(target_list_shuffled))+" learned")


results_file.write("Maximum number of syllables: "+str(syll_num)+"\n")

results_file.write("Grammar changed "+str(change_counter)+"/"+str(datum_counter)+" times\n")

results_file.write("Learned grammar:\n")
ranked_constraints_post = ranking(constraint_dict)
for const in ranked_constraints_post:
    results_file.write(const+"\t"+str(constraint_dict[const])+"\n")

endtime = datetime.datetime.now()
duration = endtime-starttime
results_file.write("Time taken: "+str(duration))

print("Time taken: "+str(duration))
print("Output file: "+result_file_name[1:])

results_file.close()


### Plotting
plt.subplot(2, 1, 1)  
for const in constraint_dict.keys():
    plt.plot(iteration_track, trend_tracks[const], label=str(const))
    #plt.xscale('log')

#labelLines(plt.gca().get_lines(), align=False, fontsize=12)

i = 0
yticks_learning = []
while i < len(learned_success_list):
    if math.floor(len(learned_success_list)*0.25) == float(i):
        yticks_learning.append(i)
    elif math.floor(len(learned_success_list)*0.5) == float(i):
        yticks_learning.append(i)
    elif math.floor(len(learned_success_list)*0.75) == float(i):
        yticks_learning.append(i)
    else:
        pass

    i += 1

yticks_learning.append(len(learned_success_list))

plt.subplot(2, 1, 2)
plt.plot(iteration_track, learning_track)
#plt.xscale('log')
plt.yticks(yticks_learning)
# y-axis for learning track should be 1, 2, ..., num_of_datum_tokens

fig_path = result_file_path[:-4]+".pdf"
plt.savefig(fig_path)
plt.show()
